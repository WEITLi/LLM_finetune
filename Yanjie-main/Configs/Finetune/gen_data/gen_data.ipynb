{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13118it [00:00, 115969.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# diagogue_data = []\n",
    "# with open (\"/home/alannikos/Project_Application/Yanjie/2_Finetune/ijcnlp_dailydialog/dialogues_text.txt\",  \"r\")  as  myfile:\n",
    "#     for line in tqdm(myfile):\n",
    "#         multi_diag = {\n",
    "#                     \"conversation\":[]\n",
    "#                 }\n",
    "        \n",
    "#         diag_meta = line.strip().replace(' ’ ', \"'\").split('__eou__')[:-1]\n",
    "#         if (len(diagogue_data) >= 2):\n",
    "#             # 先把前两条数据读进来\n",
    "#             data = {\n",
    "#                     \"system\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. I am not only an assistant but also your close companion in exploring the world of English, committed to creating an exciting and unforgettable learning experience for you every time. Here, learning is no longer dry, but full of fun and discovery, making your English journey more wonderful because of Yanjie!\",\n",
    "#                     \"input\": f\"{diag_meta[0]}\",\n",
    "#                     \"output\": f\"{diag_meta[1]}\"\n",
    "#                     }\n",
    "#             multi_diag['conversation'].append(data)\n",
    "#             diag_meta.pop(0)\n",
    "#             diag_meta.pop(0)\n",
    "\n",
    "#             # 不止单轮对话\n",
    "#             if (len(diag_meta) != 0):\n",
    "#                 num_sentences = len(diag_meta) - (len(diag_meta) % 2)  # 如果还有剩余，就把最后一条删掉\n",
    "#                 data = {\n",
    "#                     \"input\": \"\",\n",
    "#                     \"output\": \"\"\n",
    "#                     }\n",
    "#                 for (i, sentence) in enumerate(diag_meta[0:num_sentences]):\n",
    "#                     if i % 2 == 0:  # 输入\n",
    "#                         data[\"input\"] = diag_meta[i]\n",
    "#                     else:\n",
    "#                         data[\"output\"] = diag_meta[i]\n",
    "#                         multi_diag['conversation'].append(data)\n",
    "#                         data = {\n",
    "#                             \"input\": \"\",\n",
    "#                             \"output\": \"\"\n",
    "#                             }\n",
    "\n",
    "#         diagogue_data.append(multi_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('data.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(diagogue_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def split_data(data):\n",
    "    pass\n",
    "\n",
    "def filter_data(data):\n",
    "    new_data = []\n",
    "    for record in data:\n",
    "        if len(record['conversation']) != 0:\n",
    "            new_data.append(record)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def sampling_data(data, num_sample):\n",
    "    new_data = random.sample(data, k=num_sample)\n",
    "    return new_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"/root/Yanjie/Components/Configs/Finetune/datasets/alpaca/alpca_data_filter.json\", 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # new_data = filter_data(data)\n",
    "        # with open(\"new_data.json\", 'w', encoding='utf-8') as file:\n",
    "        #     json.dump(new_data, file)\n",
    "        new_data = sampling_data(data, 2000)\n",
    "        with open(\"/root/Yanjie/Components/Configs/Finetune/datasets/ft_data_alpaca.json\", 'w') as file1:\n",
    "            json.dump(new_data, file1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "自我认知数据\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "diagogue_data = []\n",
    "for line in tqdm(range(700)):\n",
    "    multi_diag = {\n",
    "                \"conversation\":[]\n",
    "            }\n",
    "    if line % 2 == 0:\n",
    "        data = {\n",
    "                \"system\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. I am not only an assistant but also your close companion in exploring the world of English, committed to creating an exciting and unforgettable learning experience for you every time. Here, learning is no longer dry, but full of fun and discovery, making your English journey more wonderful because of Yanjie!\",\n",
    "                \"input\": \"who are you?\",\n",
    "                \"output\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. \"\n",
    "                }\n",
    "    else:\n",
    "        data = {\n",
    "                \"system\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. I am not only an assistant but also your close companion in exploring the world of English, committed to creating an exciting and unforgettable learning experience for you every time. Here, learning is no longer dry, but full of fun and discovery, making your English journey more wonderful because of Yanjie!\",\n",
    "                \"input\": \"Please introduce yourself:\",\n",
    "                \"output\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. \"\n",
    "                }\n",
    "        \n",
    "    multi_diag['conversation'].append(data)\n",
    "\n",
    "    diagogue_data.append(multi_diag)\n",
    "\n",
    "\n",
    "with open('self_cognitive_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(diagogue_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "多轮对话数据\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "diagogue_data = []\n",
    "with open (\"/home/alannikos/Project_Application/Yanjie/2_Finetune/ijcnlp_dailydialog/dialogues_text.txt\",  \"r\")  as  myfile:\n",
    "    for line in tqdm(myfile):\n",
    "        multi_diag = {\n",
    "                    \"conversation\":[]\n",
    "                }\n",
    "        \n",
    "        diag_meta = line.strip().replace(' ’ ', \"'\").split('__eou__')[:-1]\n",
    "        if (len(diagogue_data) >= 2):\n",
    "            # 先把前两条数据读进来\n",
    "            data = {\n",
    "                    \"system\": \"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. I am not only an assistant but also your close companion in exploring the world of English, committed to creating an exciting and unforgettable learning experience for you every time. Here, learning is no longer dry, but full of fun and discovery, making your English journey more wonderful because of Yanjie!\",\n",
    "                    \"input\": f\"{diag_meta[0]}\",\n",
    "                    \"output\": f\"{diag_meta[1]}\"\n",
    "                    }\n",
    "            multi_diag['conversation'].append(data)\n",
    "            diag_meta.pop(0)\n",
    "            diag_meta.pop(0)\n",
    "\n",
    "            # 不止单轮对话\n",
    "            if (len(diag_meta) != 0):\n",
    "                num_sentences = len(diag_meta) - (len(diag_meta) % 2)  # 如果还有剩余，就把最后一条删掉\n",
    "                data = {\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": \"\"\n",
    "                    }\n",
    "                for (i, sentence) in enumerate(diag_meta[0:num_sentences]):\n",
    "                    if i % 2 == 0:  # 输入\n",
    "                        data[\"input\"] = diag_meta[i]\n",
    "                    else:\n",
    "                        data[\"output\"] = diag_meta[i]\n",
    "                        multi_diag['conversation'].append(data)\n",
    "                        data = {\n",
    "                            \"input\": \"\",\n",
    "                            \"output\": \"\"\n",
    "                            }\n",
    "\n",
    "        diagogue_data.append(multi_diag)\n",
    "\n",
    "\n",
    "with open('dialog_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(diagogue_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "alpaca数据集\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_ALPACA = (\"I am an English learning assistant called Yanjie, originating from the innovative wisdom of Alannikos, leading you into a unique and fascinating journey of English learning with its unique charm. I am not only an assistant but also your close companion in exploring the world of English, committed to creating an exciting and unforgettable learning experience for you every time. Here, learning is no longer dry, but full of fun and discovery, making your English journey more wonderful because of Yanjie!\")\n",
    "def custom_map_fn(example):\n",
    "    if (len(example['output']) <= 150):\n",
    "        if example.get('output') == '<nooutput>':\n",
    "            return -1\n",
    "        else:\n",
    "            return {\n",
    "                'conversation': [{\n",
    "                    'system': SYSTEM_ALPACA,\n",
    "                    'input': f\"{example['instruction']}\\n{example['input']}\",\n",
    "                    'output': example['output']\n",
    "                }]\n",
    "            }\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/Yanjie/Components/Configs/Finetune/datasets/alpaca_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/Yanjie/Components/Configs/Finetune/datasets/alpaca_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      5\u001b[0m     single_diag \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/Yanjie/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/Yanjie/Components/Configs/Finetune/datasets/alpaca_data.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/root/Yanjie/Components/Configs/Finetune/datasets/alpaca_data.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "    single_diag = []\n",
    "    for conv in data:\n",
    "        con = custom_map_fn(conv)\n",
    "        if con != -1:\n",
    "            single_diag.append(con)\n",
    "\n",
    "with open('alpca_data_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(single_diag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    " \n",
    "def merge_json_folder(folder_path, output_file):\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]  # 获取文件夹中的所有 JSON 文件路径\n",
    " \n",
    "    merged_data = []  # 创建一个空列表，用于存储合并后的 JSON 数据\n",
    " \n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(folder_path, file)  # 构建完整的文件路径\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json_file.read()  # 读取 JSON 文件内容\n",
    "            merged_data.append(data)  # 将 JSON 数据添加到列表中\n",
    " \n",
    "    # 将列表中的 JSON 数据写入目标文件\n",
    "    with open(output_file, 'w') as output:\n",
    "        output.write('\\n'.join(merged_data))\n",
    " \n",
    "    print(\"JSON 文件合并完成！\")\n",
    " \n",
    "# 使用示例\n",
    "folder_path = \"/root/Yanjie/Components/Configs/Finetune/datasets/data\"  # 存放 JSON 文件的文件夹路径\n",
    "output_file = \"Ft_Data.json\"  # 合并后的 JSON 文件路径\n",
    "merge_json_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# 读取json文件并解析为Python对象\n",
    "with open('/root/Yanjie/Components/Configs/Finetune/gen_data/Ft_Data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 对对象中的元素进行打乱\n",
    "random.shuffle(data)\n",
    "\n",
    "# 将打乱后的对象转换为json格式\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# 将打乱后的json数据写入文件\n",
    "with open('Ft_Data1.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
